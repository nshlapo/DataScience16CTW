{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy import ndimage\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import manifold, datasets\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from matplotlib.patches import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur_dir = os.path.dirname('__file__')\n",
    "data = pd.read_csv(os.path.join(cur_dir, \"WDI_Data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    \"\"\"Create a 21st column, which is the average of a certain stat since the year 2000.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['21st'] = df.ix[:, '2000':].mean(axis=1)\n",
    "    df['Obama'] = df.ix[:, '2011':'2015'].mean(axis=1)\n",
    "    df['Reagan'] = df.ix[:, '1983':'1988'].mean(axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_year(data, year='21st'):\n",
    "    \"\"\"Averages all statistics over the course of the 21st century, and create a dataframe where each\n",
    "    row is a country, and each column is a statistic.\n",
    "    \"\"\"\n",
    "    grouped = data.groupby('Indicator Name')\n",
    "    indic_dict = {}\n",
    "    for indicator, group in grouped:\n",
    "        for index, row in group.iterrows():\n",
    "            if indicator not in indic_dict:\n",
    "                indic_dict[indicator] = []\n",
    "            indic_dict[indicator].append(row[year])\n",
    "    for indicator, group in grouped:\n",
    "        names = group['Country Name'].tolist()\n",
    "        break   \n",
    "        \n",
    "    df = pd.DataFrame(indic_dict, index=names)\n",
    "    return df\n",
    "\n",
    "\n",
    "def only_countries(df, n=None, data_thresh=0):\n",
    "    \"\"\"Filter the input df and remove all rows that do not represent a country. (For example, there is one row the\n",
    "    World.)\"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "    words = ['World', 'income', '(developing only)', 'OECD', 'countries', 'Euro', 'Asia', 'America', 'situations', 'states']\n",
    "    sel = []\n",
    "    for i in df.index:\n",
    "        temp = filled_pct(df.loc[i]) > data_thresh\n",
    "        for w in words:\n",
    "            if w in i:\n",
    "                temp = False\n",
    "        sel.append(temp)\n",
    "        \n",
    "    return df[sel].iloc[:n]\n",
    "\n",
    "def few_na_cols(df, thresh=0, required_countries=[]):\n",
    "    \"\"\"Remove all columns that do not have thresh proportion of values filled in. required_countries are countries that\n",
    "    must have a value in a column for it to be kept.\"\"\"\n",
    "    res = []\n",
    "    for c in df.columns:\n",
    "        perc = filled_pct(df[c])\n",
    "        meets_required = True\n",
    "        for count in required_countries:\n",
    "            if df[c].isnull().loc[count]:\n",
    "                meets_required = False\n",
    "        if perc > thresh and meets_required:\n",
    "            res.append(c)\n",
    "    return df[res]\n",
    "\n",
    "def highest_pop_df(df, n=None, by='Population, total'):\n",
    "    \"\"\"Return the dataframe, sorted by population.\"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "    df = df.sort_values(by,ascending=False)\n",
    "    return df.iloc[:n]\n",
    "\n",
    "def filled_pct(col):\n",
    "    return float(col.notnull().sum()) / len(col)\n",
    "\n",
    "def get_unique_cols(df, thresh=0.95):\n",
    "    uniques = df.columns.tolist()\n",
    "    corr_df = df.corr()\n",
    "    i = 0\n",
    "    while i < len(uniques):\n",
    "        j = 0\n",
    "        while j < len(uniques):\n",
    "            if i != j:\n",
    "                col1 = uniques[i]\n",
    "                col2 = uniques[j]\n",
    "                r_sq = corr_df[col1].loc[col2] ** 2\n",
    "                if r_sq >= thresh:\n",
    "                    if filled_pct(df[col1]) > filled_pct(df[col2]):\n",
    "                        drop_index = j\n",
    "                    elif filled_pct(df[col1]) < filled_pct(df[col2]):\n",
    "                        drop_index = i\n",
    "                    else:\n",
    "                        drop_index = i\n",
    "                    uniques.pop(drop_index)\n",
    "                    if drop_index == i:\n",
    "                        i = i - 1\n",
    "                        break\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return uniques\n",
    "                    \n",
    "                \n",
    "    corr = df.corr()\n",
    "    for col in corr:\n",
    "        corr[col].loc[col]\n",
    "        break\n",
    "        \n",
    "\n",
    "def get_data_availability(df):\n",
    "    \"\"\"Get a dataframe containing the percentage of rows for each column that is not NaN\"\"\"\n",
    "    null_pcts = []\n",
    "\n",
    "    for column in df.columns:\n",
    "        col = df[column]\n",
    "        n = filled_pct(col)\n",
    "        null_pcts.append(n)\n",
    "\n",
    "\n",
    "    temp = pd.DataFrame(data=null_pcts, index=df.columns,\n",
    "                        columns=['Data Availability']).sort_values('Data Availability', ascending=False)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_group_csv(csv_dwnld, save_to):\n",
    "    \"\"\"Call this function after downloading a csv for a single country, for a single year, for a group of indicators.\n",
    "    This function exports the indicator names to a csv.\"\"\"\n",
    "\n",
    "    data = pd.read_csv(os.path.join(cur_dir, csv_dwnld))\n",
    "    education_stats = data['Series Name'].tolist()\n",
    "\n",
    "    with open(save_to, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(education_stats)\n",
    "        \n",
    "def get_indicator_list_from_csv(csv_file):\n",
    "    \"\"\"And then this gets the corresponding indicator list from the csv.\"\"\"\n",
    "    with open(csv_file) as f:\n",
    "        reader = csv.reader(f)\n",
    "        your_list = list(reader)\n",
    "    l = your_list[0]\n",
    "    l = [s for s in l if s != \"nan\"]\n",
    "    return l\n",
    "\n",
    "\"\"\"\n",
    "# example usage\n",
    "create_group_csv('Data_Extract_From_World_Development_Indicators_Data.csv', 'health.csv')\n",
    "health_indicators = get_indicator_list_from_csv('health.csv')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_label_dict(countries, clustering):\n",
    "    \"\"\"Takes in a list of countries and a cluster, and returns a dictionary mapping from cluster to a list\n",
    "    of countries in that cluster\"\"\"\n",
    "    label_dict = {}\n",
    "\n",
    "    for index, label in enumerate(clustering.labels_):\n",
    "        label_dict[countries[index]] = label\n",
    "    return label_dict\n",
    "\n",
    "def filter_df(df, features, n=None, country_data_thresh=0, column_data_thresh=.8):\n",
    "    highest_pop = highest_pop_df(df)[features]\n",
    "    highest_pop = highest_pop[get_unique_cols(highest_pop)]\n",
    "\n",
    "    countries = only_countries(highest_pop, n=n, data_thresh=country_data_thresh)\n",
    "    economics = few_na_cols(countries, thresh=column_data_thresh)\n",
    "\n",
    "    sample = economics.fillna(economics.mean())\n",
    "    return sample\n",
    "\n",
    "\n",
    "\n",
    "def cluster_highest_pop_countries(df, filter=False, features=None, n=None, m=3, country_data_thresh=0, column_data_thresh=.8):\n",
    "    \"\"\"This helper function clusters the highest n populated countries into m clusters. It returns the cluster, a\n",
    "    cluster dictionary creating using cluster_label_dict(), X_red - a spectrical embedding of the data into two dimensions,\n",
    "    and the list of countries.\"\"\"\n",
    "    \n",
    "    # if we need to filter out rows and columns with too many NaNs\n",
    "    if filter:\n",
    "        if features is None:\n",
    "            features = df.columns\n",
    "        sample = filter_df(df, features, n=n, country_data_thresh=country_data_thresh,\n",
    "                           column_data_thresh=column_data_thresh)\n",
    "    else:\n",
    "        sample = df\n",
    "        \n",
    "    countries = sample.index\n",
    "    \n",
    "    # convert to a matrix and standardize it\n",
    "    X = sample.as_matrix()\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # spectrally embed the matrix down to one axis in order to later reorder the clusters so that lower numbers\n",
    "    # correspond to poorer countries, and high numbers correspond to richer ones.\n",
    "    X_red = manifold.SpectralEmbedding(n_components=1).fit_transform(X)\n",
    "    position_dic = {}\n",
    "    for i in range(len(countries)):\n",
    "        position_dic[countries[i]] = X_red[i, 0]\n",
    "        \n",
    "    # specreally embed to 2-d in order to plot later\n",
    "    X_red = manifold.SpectralEmbedding(n_components=2).fit_transform(X)\n",
    "    \n",
    "    print \"Clustering using %i columns for %i countries...\" % (sample.shape[1], sample.shape[0])\n",
    "\n",
    "    # perform agglomerative clustering\n",
    "    clustering = AgglomerativeClustering(linkage='ward', n_clusters=m)\n",
    "    clustering.fit(X)\n",
    "    \n",
    "    # convert the cluster into a dictionary mapping from country to its cluster number\n",
    "    clust_dict = cluster_label_dict(countries, clustering)\n",
    "    \n",
    "    # reorder the clusters so that lower clusters are poorer countries\n",
    "    reorder_dictionary(clust_dict, position_dic)\n",
    "    \n",
    "    # return everything we'll need later\n",
    "    return sample, clustering, clust_dict, X_red, countries, position_dic\n",
    "\n",
    "def plot_clustering(X_red, clustering, countries, title=None, reverse=False, save_name=''):\n",
    "    \"\"\"Plots a clustering of countries. Takes in X_red, the cluster object, and the list of countries.\"\"\"\n",
    "    labels = clustering.labels_\n",
    "    if reverse:\n",
    "        labels = [max(labels) - l + min(labels) for l in labels]\n",
    "    x_min, x_max = np.min(X_red, axis=0), np.max(X_red, axis=0)\n",
    "    X_red = (X_red - x_min) / (x_max - x_min)\n",
    "    us_index = -1\n",
    "    for i in range(X_red.shape[0]):\n",
    "        if (countries[i] == 'United States'):\n",
    "            us_index = i\n",
    "            continue\n",
    "        plt.text(X_red[i, 0], X_red[i, 1], str(countries[i])[:3],\n",
    "                 color=map_to_color(labels[i], min(labels), max(labels)),\n",
    "                 fontdict={'weight': 'bold', 'size': 15})\n",
    "        \n",
    "    if (us_index != -1):\n",
    "        plt.text(X_red[us_index, 0], X_red[us_index, 1], 'U.S.',\n",
    "                 color='black',\n",
    "                 fontdict={'weight': 'bold', 'size': 25})\n",
    "\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title, size=17)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    if save_name: plt.gcf().savefig(os.path.join(cur_dir, 'img', save_name + \".eps\"))\n",
    "    \n",
    "def map_to_color(label, label_min, label_max):\n",
    "    fraction = float(label - label_min) / (label_max - label_min)\n",
    "    mapped = fraction\n",
    "    return plt.get_cmap('Spectral')(mapped)\n",
    "    \n",
    "def remap_countries(country_list):\n",
    "    d = {'Iran (Islamic Republic of)': 'Iran, Islamic Rep.', 'Burma': 'Myanmar',\n",
    "         \"Lao People's Democratic Republic\": \"Lao PDR\", 'Cape Verde': \"Cabo Verde\", 'Saint Lucia': 'St. Lucia',\n",
    "         \"Slovakia\": \"Slovak Republic\", \"Saint Vincent and the Grenadines\": \"St. Vincent and the Grenadines\",\n",
    "         \"Kyrgyzstan\": \"Kyrgyz Republic\", \"The former Yugoslav Republic of Macedonia\": \"Macedonia, FYR\",\n",
    "         \"Korea, Democratic People's Republic of\": \"Korea, Dem. Rep.\", \"Korea, Republic of\": \"Korea, Rep.\"\n",
    "        }\n",
    "    temp = []\n",
    "    for c in country_list:\n",
    "        if c in d:\n",
    "            temp.append(d[c])\n",
    "        else:\n",
    "            temp.append(c)\n",
    "    return temp\n",
    "    \n",
    "def plot_map(clust_dict, reverse=False, save_name=''):\n",
    "    '''Use dict of labels and clusters of countries to create a world map color-coded by label'''\n",
    "    \n",
    "    #Create map object and load world countries Shapefiles\n",
    "    m = Basemap(projection='mill')\n",
    "    m.readshapefile('Borders/world', name='countries', drawbounds=True)\n",
    "    \n",
    "    # Extract the country names from Basemap. Each country appears once for each of its polygons\n",
    "    country_names = [shape_dict['NAME'] for shape_dict in m.countries_info]\n",
    "    # remap the names of the countries to match the names of the countries in the World Bank DataBank\n",
    "    country_names = remap_countries(country_names)\n",
    "    ax = plt.gca()\n",
    "    labels = [label for country, label in clust_dict.iteritems()]\n",
    "    \n",
    "    # clust_dict is a dictionary mapping from country name to its cluster number\n",
    "    for country, label in clust_dict.iteritems():\n",
    "        # if the clusters are in the wrong orientation (Africa is Blue, Europe is Red), invert them\n",
    "        if reverse:\n",
    "            label = max(labels) - label + min(labels)\n",
    "        found = False\n",
    "        # convert the cluster number to a matplotlib color\n",
    "        color = map_to_color(float(label), min(labels), max(labels))\n",
    "        #Countries with non-contiguous landmasses are constructed as multiple polygons\n",
    "        \n",
    "        # create a list of indicies of where this country occurrs in basemap\n",
    "        country_indices = []\n",
    "        for i, c in enumerate(country_names):\n",
    "            if c.lower() in country.lower() or country.lower() in c.lower():\n",
    "                country_indices.append(i)\n",
    "                found = True\n",
    "                \n",
    "        if not found:\n",
    "            print country + \" was not found!\"\n",
    "        \n",
    "        # plot the country's polygons\n",
    "        segs = [m.countries[index] for index in country_indices]\n",
    "        polys = [Polygon(seg, facecolor=color, edgecolor=color) for seg in segs]\n",
    "        [ax.add_patch(poly) for poly in polys]\n",
    "    plt.gcf().set_size_inches(20,10)\n",
    "    if save_name: plt.gcf().savefig(os.path.join(cur_dir, 'img', save_name + \".eps\"))\n",
    "    \n",
    "    plt.show()\n",
    "    return country_names\n",
    "\n",
    "def reorder_dictionary(clust_dic, pos_dic):\n",
    "    clust = pd.DataFrame.from_dict({'group': clust_dic, 'one_d': pos_dic})\n",
    "    means = clust.groupby('group').mean()\n",
    "    means = means.sort_values('one_d', ascending=False)\n",
    "    ind = means.index.tolist()\n",
    "    for key, value in clust_dic.iteritems():\n",
    "        clust_dic[key] = ind.index(value)\n",
    "\n",
    "def cluster_means(dic, df):\n",
    "    df = df.copy()\n",
    "    df2 = df.copy()\n",
    "    df['Name'] = df.index\n",
    "    \n",
    "    X = StandardScaler().fit_transform(df2.as_matrix())\n",
    "    df2 = pd.DataFrame(data=X, index=df2.index, columns=df2.columns)\n",
    "    \n",
    "    df2['Group'] = df.Name.map(dic)\n",
    "    \n",
    "    means = {},\n",
    "    for name, group in df2.groupby('Group'):\n",
    "        means[name] = group.mean()\n",
    "        \n",
    "    return means\n",
    "\n",
    "    \n",
    "def mean_diffs(means_dic1, group1, group2):\n",
    "    diff = (means_dic1[group1] - means_dic1[group2])\n",
    "    temp = pd.DataFrame({'dif': diff})\n",
    "    temp['abs_val'] = temp.dif.abs()\n",
    "    temp = temp.sort_values('abs_val', ascending=False)\n",
    "    return temp.dif\n",
    "\n",
    "def feature_intersection(df1, df2):\n",
    "    return list(set(df1.columns).intersection(set(df2.columns)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = preprocess(data) # create the 21st column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = one_year(data, year='21st')\n",
    "Reagan = one_year(data, year='Reagan') # create a dataframe using the 21st columns\n",
    "Obama = one_year(data, year='Obama') # create a dataframe using the 21st columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create our feature lists\n",
    "\n",
    "features = {}\n",
    "groups = ['health', 'education', 'economics', 'environment']\n",
    "\n",
    "for g in groups:\n",
    "    features[g] = get_indicator_list_from_csv(g + '.csv')\n",
    "    if 'Population, total' in features[g]:\n",
    "        features[g].remove('Population, total')\n",
    "    \n",
    "features['all'] = df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = 3\n",
    "feat='economics'\n",
    "\n",
    "res_df_r = filter_df(Reagan, features[feat], country_data_thresh=0.2, column_data_thresh=0.6)\n",
    "res_df_o = filter_df(Obama, features[feat], country_data_thresh=0.2, column_data_thresh=0.6)\n",
    "feat_int = feature_intersection(res_df_r, res_df_o)\n",
    "\n",
    "res_df_r, clust_r, dic_r, X_red_r, countries_r, pos_dic_r = \\\n",
    "    cluster_highest_pop_countries(res_df_r[feat_int], m=clusters)\n",
    "res_df_o, clust_o, dic_o, X_red_o, countries_o, pos_dic_o = \\\n",
    "    cluster_highest_pop_countries(res_df_o[feat_int], m=clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reverse_r = False\n",
    "reverse_o = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_clustering(X_red_r, clust_r, countries_r, reverse=reverse_r)\n",
    "plt.gcf().set_size_inches(6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_clustering(X_red_o, clust_o, countries_o, reverse=reverse_o)\n",
    "plt.gcf().set_size_inches(6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries_mapped = plot_map(dic_r, reverse=reverse_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "countries_mapped = plot_map(dic_o, reverse=reverse_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Difference between extremes - Obama\"\n",
    "means_r = cluster_means(dic_o, res_df_o)\n",
    "m_d = mean_diffs(means_r, 2, 0)\n",
    "print m_d[m_d.index != 'Group'][0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
