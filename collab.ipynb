{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy import ndimage\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import manifold, datasets\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from matplotlib.patches import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur_dir = os.path.dirname('__file__')\n",
    "data = pd.read_csv(os.path.join(cur_dir, \"WDI_Data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    \"\"\"Create a 21st column, which is the average of a certain stat since the year 2000.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['21st'] = df.ix[:, '2000':].mean(axis=1)\n",
    "    df['Obama'] = df.ix[:, '2011':'2015'].mean(axis=1)\n",
    "    df['Reagan'] = df.ix[:, '1983':'1988'].mean(axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_year(data, year='21st'):\n",
    "    \"\"\"Averages all statistics over the course of the 21st century, and create a dataframe where each\n",
    "    row is a country, and each column is a statistic.\n",
    "    \"\"\"\n",
    "    grouped = data.groupby('Indicator Name')\n",
    "    indic_dict = {}\n",
    "    for indicator, group in grouped:\n",
    "        for index, row in group.iterrows():\n",
    "            if indicator not in indic_dict:\n",
    "                indic_dict[indicator] = []\n",
    "            indic_dict[indicator].append(row[year])\n",
    "    for indicator, group in grouped:\n",
    "        names = group['Country Name'].tolist()\n",
    "        break   \n",
    "        \n",
    "    return pd.DataFrame(indic_dict, index=names)\n",
    "\n",
    "\n",
    "def only_countries(df, n=None, data_thresh=0):\n",
    "    \"\"\"Filter the input df and remove all rows that do not represent a country. (For example, there is one row the\n",
    "    World.)\"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "    words = ['World', 'income', '(developing only)', 'OECD', 'countries', 'Euro', 'Asia', 'America', 'situations', 'states']\n",
    "    sel = []\n",
    "    for i in df.index:\n",
    "        temp = filled_pct(df.loc[i]) >= data_thresh\n",
    "        for w in words:\n",
    "            if w in i:\n",
    "                temp = False\n",
    "        sel.append(temp)\n",
    "        \n",
    "    return df[sel].iloc[:n]\n",
    "\n",
    "def few_na_cols(df, thresh=0, required_countries=[]):\n",
    "    \"\"\"Remove all columns that do not have thresh proportion of values filled in. required_countries are countries that\n",
    "    must have a value in a column for it to be kept.\"\"\"\n",
    "    res = []\n",
    "    for c in df.columns:\n",
    "        perc = filled_pct(df[c])\n",
    "        meets_required = True\n",
    "        for count in required_countries:\n",
    "            if df[c].isnull().loc[count]:\n",
    "                meets_required = False\n",
    "        if perc >= thresh and meets_required:\n",
    "            res.append(c)\n",
    "    return df[res]\n",
    "\n",
    "def highest_pop_df(df, n=None, by='Population, total'):\n",
    "    \"\"\"Return the dataframe, sorted by population.\"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "    df = df.sort_values(by,ascending=False)\n",
    "    return df.iloc[:n]\n",
    "\n",
    "def filled_pct(col):\n",
    "    return float(col.notnull().sum()) / len(col)\n",
    "\n",
    "def get_unique_cols(df, thresh=0.95):\n",
    "    uniques = df.columns.tolist()\n",
    "    corr_df = df.corr()\n",
    "    i = 0\n",
    "    while i < len(uniques):\n",
    "        j = 0\n",
    "        while j < len(uniques):\n",
    "            if i != j:\n",
    "                col1 = uniques[i]\n",
    "                col2 = uniques[j]\n",
    "                r_sq = corr_df[col1].loc[col2] ** 2\n",
    "                if r_sq >= thresh:\n",
    "                    if filled_pct(df[col1]) > filled_pct(df[col2]):\n",
    "                        drop_index = j\n",
    "                    elif filled_pct(df[col1]) < filled_pct(df[col2]):\n",
    "                        drop_index = i\n",
    "                    else:\n",
    "                        drop_index = random.choice([i, j])\n",
    "                    uniques.pop(drop_index)\n",
    "                    if drop_index == i:\n",
    "                        i = i - 1\n",
    "                        break\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return uniques\n",
    "                    \n",
    "                \n",
    "    corr = df.corr()\n",
    "    for col in corr:\n",
    "        corr[col].loc[col]\n",
    "        break\n",
    "        \n",
    "\n",
    "def get_data_availability(df):\n",
    "    \"\"\"Get a dataframe containing the percentage of rows for each column that is not NaN\"\"\"\n",
    "    null_pcts = []\n",
    "\n",
    "    for column in df.columns:\n",
    "        col = df[column]\n",
    "        n = filled_pct(col)\n",
    "        null_pcts.append(n)\n",
    "\n",
    "\n",
    "    temp = pd.DataFrame(data=null_pcts, index=df.columns,\n",
    "                        columns=['Data Availability']).sort_values('Data Availability', ascending=False)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_group_csv(csv_dwnld, save_to):\n",
    "    \"\"\"Call this function after downloading a csv for a single country, for a single year, for a group of indicators.\n",
    "    This function exports the indicator names to a csv.\"\"\"\n",
    "\n",
    "    data = pd.read_csv(os.path.join(cur_dir, csv_dwnld))\n",
    "    education_stats = data['Series Name'].tolist()\n",
    "\n",
    "    with open(save_to, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(education_stats)\n",
    "        \n",
    "def get_indicator_list_from_csv(csv_file):\n",
    "    \"\"\"And then this gets the corresponding indicator list from the csv.\"\"\"\n",
    "    with open(csv_file) as f:\n",
    "        reader = csv.reader(f)\n",
    "        your_list = list(reader)\n",
    "    l = your_list[0]\n",
    "    l = [s for s in l if s != \"nan\"]\n",
    "    return l\n",
    "\n",
    "\"\"\"\n",
    "# example usage\n",
    "create_group_csv('Data_Extract_From_World_Development_Indicators_Data.csv', 'health.csv')\n",
    "health_indicators = get_indicator_list_from_csv('health.csv')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_label_dict(countries, clustering):\n",
    "    \"\"\"Takes in a list of countries and a cluster, and returns a dictionary mapping from cluster to a list\n",
    "    of countries in that cluster\"\"\"\n",
    "    label_dict = {}\n",
    "\n",
    "    for index, label in enumerate(clustering.labels_):\n",
    "        label_dict[countries[index]] = label\n",
    "    return label_dict\n",
    "\n",
    "def cluster_highest_pop_countries(df, features, n=30, m=3, country_data_thresh=0, column_data_thresh=.8):\n",
    "    \"\"\"This helper function clusters the highest n populated countries into m clusters. It returns the cluster, a\n",
    "    cluster dictionary creating using cluster_label_dict(), X_red - a spectrical embedding of the data into two dimensions,\n",
    "    and the list of countries.\"\"\"\n",
    "    \n",
    "    highest_pop = highest_pop_df(df)[features]\n",
    "    highest_pop = highest_pop[get_unique_cols(highest_pop)]\n",
    "\n",
    "    countries = only_countries(highest_pop, n=n, data_thresh=country_data_thresh)\n",
    "    economics = few_na_cols(countries, thresh=column_data_thresh)\n",
    "\n",
    "    sample = economics.fillna(economics.mean())\n",
    "    countries = sample.index\n",
    "    \n",
    "    X = sample.as_matrix()\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    X_red = manifold.SpectralEmbedding(n_components=1).fit_transform(X)\n",
    "    position_dic = {}\n",
    "    for i in range(len(countries)):\n",
    "        position_dic[countries[i]] = X_red[i, 0]\n",
    "    X_red = manifold.SpectralEmbedding(n_components=2).fit_transform(X)\n",
    "\n",
    "    clustering = AgglomerativeClustering(linkage='ward', n_clusters=m)\n",
    "    clustering.fit(X_red)\n",
    "    clust_dict = cluster_label_dict(countries, clustering)\n",
    "    \n",
    "    return sample, clustering, clust_dict, X_red, countries, position_dic\n",
    "\n",
    "def plot_clustering(X_red, clustering, countries, title=None):\n",
    "    \"\"\"Plots a clustering of countries. Takes in X_red, the cluster object, and the list of countries.\"\"\"\n",
    "    labels = clustering.labels_\n",
    "    x_min, x_max = np.min(X_red, axis=0), np.max(X_red, axis=0)\n",
    "    X_red = (X_red - x_min) / (x_max - x_min)\n",
    "    us_index = -1\n",
    "    for i in range(X_red.shape[0]):\n",
    "        if (countries[i] == 'United States'):\n",
    "            us_index = i\n",
    "            continue\n",
    "        plt.text(X_red[i, 0], X_red[i, 1], str(countries[i])[:3],\n",
    "                 color=map_to_color(labels[i], min(labels), max(labels)),\n",
    "                 fontdict={'weight': 'bold', 'size': 15})\n",
    "        \n",
    "    if (us_index != -1):\n",
    "        plt.text(X_red[us_index, 0], X_red[us_index, 1], 'U.S.',\n",
    "                 color='black',\n",
    "                 fontdict={'weight': 'bold', 'size': 25})\n",
    "\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title, size=17)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "def map_to_color(label, label_min, label_max):\n",
    "    fraction = float(label - label_min) / (label_max - label_min)\n",
    "    mapped = fraction\n",
    "    return plt.get_cmap('Spectral')(mapped)\n",
    "    \n",
    "def remap_countries(country_list):\n",
    "    d = {'Iran (Islamic Republic of)': 'Iran, Islamic Rep.', 'Burma': 'Myanmar',\n",
    "         \"Lao People's Democratic Republic\": \"Lao PDR\", 'Cape Verde': \"Cabo Verde\", 'Saint Lucia': 'St. Lucia',\n",
    "         \"Slovakia\": \"Slovak Republic\", \"Saint Vincent and the Grenadines\": \"St. Vincent and the Grenadines\",\n",
    "         \"Kyrgyzstan\": \"Kyrgyz Republic\", \"The former Yugoslav Republic of Macedonia\": \"Macedonia, FYR\",\n",
    "         \"Korea, Democratic People's Republic of\": \"Korea, Dem. Rep.\", \"Korea, Republic of\": \"Korea, Rep.\"\n",
    "        }\n",
    "    temp = []\n",
    "    for c in country_list:\n",
    "        if c in d:\n",
    "            temp.append(d[c])\n",
    "        else:\n",
    "            temp.append(c)\n",
    "    return temp\n",
    "    \n",
    "def plot_map(clust_dict):\n",
    "    '''Use dict of labels and clusters of countries to create a world map color-coded by label'''\n",
    "    \n",
    "    #Create map object and load world countries Shapefiles\n",
    "    m = Basemap(projection='mill')\n",
    "    m.readshapefile('Borders/world', name='countries', drawbounds=True)\n",
    "    \n",
    "    country_names = [shape_dict['NAME'] for shape_dict in m.countries_info]\n",
    "    country_names = remap_countries(country_names)\n",
    "    ax = plt.gca()\n",
    "    labels = [float(label) for country, label in clust_dict.iteritems()]\n",
    "            \n",
    "    for country, label in clust_dict.iteritems():\n",
    "        found = False\n",
    "        color = map_to_color(float(label), min(labels), max(labels))\n",
    "        #Countries with non-contiguous landmasses are constructed as multiple polygons\n",
    "        \n",
    "        country_indices = []\n",
    "        \n",
    "        for i, c in enumerate(country_names):\n",
    "            if c.lower() in country.lower() or country.lower() in c.lower():\n",
    "                country_indices.append(i)\n",
    "                found = True\n",
    "                \n",
    "        if not found:\n",
    "            print country + \" was not found!\"\n",
    "        \n",
    "        segs = [m.countries[index] for index in country_indices]\n",
    "        polys = [Polygon(seg, facecolor=color, edgecolor=color) for seg in segs]\n",
    "        [ax.add_patch(poly) for poly in polys]\n",
    "        \n",
    "#     m.drawmapboundary(fill_color='aqua')\n",
    "    plt.gcf().set_size_inches(20,10)\n",
    "    plt.show()\n",
    "    return country_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = preprocess(data) # create the 21st column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = one_year(data, year='21st')\n",
    "Reagan = one_year(data, year='Reagan') # create a dataframe using the 21st columns\n",
    "Obama = one_year(data, year='Obama') # create a dataframe using the 21st columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create our feature lists\n",
    "\n",
    "features = {}\n",
    "groups = ['health', 'education', 'economics', 'environment']\n",
    "\n",
    "for g in groups:\n",
    "    features[g] = get_indicator_list_from_csv(g + '.csv')\n",
    "    \n",
    "features['all'] = df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = 4\n",
    "feat='health'\n",
    "\n",
    "res_df_r, clust_r, dic_r, X_red_r, countries_r, pos_dic_r = \\\n",
    "    cluster_highest_pop_countries(Reagan, features[feat], n=len(df), m=clusters)\n",
    "res_df_o, clust_o, dic_o, X_red_o, countries_o, pos_dic_o = \\\n",
    "    cluster_highest_pop_countries(Obama, features[feat], n=len(df), m=clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_clustering(X_red, clust, countries, title=None)\n",
    "plt.gcf().set_size_inches(16, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries_mapped = plot_map(dic_r)\n",
    "plt.gcf().set_size_inches(16, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries_mapped = plot_map(dic_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
